import jax
import jax.numpy as jnp

from models import PPOStochasticActor, ValueNetwork
from data_structures import RunningMeanStd, Transition
# from data_structures import ReplayBuffer

def compute_gae(truncation: jnp.ndarray,
                termination: jnp.ndarray,
                rewards: jnp.ndarray,
                values: jnp.ndarray,
                bootstrap_value: jnp.ndarray,
                lambda_: float = 1.0,
                discount: float = 0.99):
    """Calculates the Generalized Advantage Estimation (GAE).

    Args:
        truncation: A float32 tensor of shape [T, B] with truncation signal.
        termination: A float32 tensor of shape [T, B] with termination signal.
        rewards: A float32 tensor of shape [T, B] containing rewards generated by
        following the behaviour policy.
        values: A float32 tensor of shape [T, B] with the value function estimates
        wrt. the target policy.
        bootstrap_value: A float32 of shape [B] with the value function estimate at
        time T.
        lambda_: Mix between 1-step (lambda_=0) and n-step (lambda_=1). Defaults to
        lambda_=1.
        discount: TD discount.

    Returns:
        A float32 tensor of shape [T, B]. Can be used as target to
        train a baseline (V(x_t) - vs_t)^2.
        A float32 tensor of shape [T, B] of advantages.
    """
    # Used everywhere, mask = (1 - d_t)
    truncation_mask = 1 - truncation
    
    # TD residuals:
    # delta_t = r_t + gamma * (1-d_t) * V(s_{t+1}) - V(s_t)
    # Append bootstrapped value to get [v1, ..., v_t+1]
    values_t_plus_1 = jnp.concatenate([values[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)
    deltas = rewards + discount * (1 - termination) * values_t_plus_1 - values
    deltas *= truncation_mask

    # Recursive Advantage calculation
    # A_t = delta_t + gamma * lambda * (1-d_t) * A_{t+1}
    # when lambda = 1 all future steps are fully considered, when lambda = 0 only 1 step TD error is considered
    # acc is the accumulated advantage
    acc = jnp.zeros_like(bootstrap_value)
    vs_minus_v_xs = []
    def compute_vs_minus_v_xs(carry, target_t):
        lambda_, acc = carry
        truncation_mask, delta, termination = target_t
        acc = delta + discount * (1 - termination) * truncation_mask * lambda_ * acc
        return (lambda_, acc), (acc)
    (_, _), (vs_minus_v_xs) = jax.lax.scan(
        compute_vs_minus_v_xs, (lambda_, acc),
        (truncation_mask, deltas, termination),
        length=int(truncation_mask.shape[0]),
        reverse=True)
    
    # Final Values and Advantages
    # advantage = (rewards + gamma * (1-termination) * V(s_{t+1})) - V(s_t)
    # Add V(x_s) to get v_s.
    vs = jnp.add(vs_minus_v_xs, values)
    vs_t_plus_1 = jnp.concatenate([vs[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)
    advantages = (rewards + discount * (1 - termination) * vs_t_plus_1 - values) * truncation_mask
    return jax.lax.stop_gradient(vs), jax.lax.stop_gradient(advantages)


def compute_ppo_loss(
        actor_network: PPOStochasticActor, # Policy network (Equinox module)
        value_network: ValueNetwork,        # Value network (Equinox module)
        observation_rms: RunningMeanStd,        # Running mean std parameters
        data,      # Transition data
        rng: jnp.array,
        entropy_cost: float = 1e-4,
        discounting: float = 0.99,
        reward_scaling: float = 1.0,
        gae_lambda: float = 0.95,
        clipping_epsilon: float = 0.2,
        normalize_advantage: bool = True
    ):

    # Put the time dimension first.
    data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)

    # Calculate the estimated advantages and values
    baseline = value_network(data.observation, normalize=observation_rms.normalize)
    bootstrap_value = value_network(data.next_observation[-1], normalize=observation_rms.normalize)
    rewards = data.reward * reward_scaling
    truncation = data.trunctation
    termination = (1 - data.discount) * (1 - truncation)
    vs, advantages = compute_gae(
        truncation=truncation,
        termination=termination,
        rewards=rewards,
        values=baseline,
        bootstrap_value=bootstrap_value,
        lambda_=gae_lambda,
        discount=discounting)
    
    # optional normalization
    if normalize_advantage:
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    # Calculate the Policy Ratio rho_s
    # rho_s = policy(a_t|s_t) / policy_old(a_t|s_t) = exp(log policy(a_t|s_t) - log policy_old(a_t|s_t))
    policy_logits = actor_network(data.observation, rms=observation_rms)
    target_action_log_probs = actor_network.log_prob(policy_logits, data.raw_action)
    behaviour_action_log_probs = data.log_prob
    rho_s = jnp.exp(target_action_log_probs - behaviour_action_log_probs)

    # Surrogate Objective: we use a clipped version of the policy ratio to prevent large updates
    # L_surrogate = min(rho_t * A_t, clip(rho_t, 1-eps, 1+eps) * A_t)
    # where rho_t * A_t: the standard policy gradient
    # where clip(rho_t, 1-eps, 1+eps) * A_t: the clipped version which constrains ratio rho_t within [1-eps, 1+eps]
    surrogate_loss1 = rho_s * advantages
    surrogate_loss2 = jnp.clip(rho_s, 1 - clipping_epsilon, 1 + clipping_epsilon) * advantages

    # The final policy loss is the negative of the mean of the clipped surrogate objective
    policy_loss = -jnp.mean(jnp.minimum(surrogate_loss1, surrogate_loss2))

    # Value function loss
    # value_loss = 1/2 * 1/T * Sum_{t=0}^T (V_s - V(s_t)) ** 2
    # the loss is scaled y an additional factor 0.5 likely to balance the loss components
    v_error = vs - baseline
    v_loss = jnp.mean(v_error ** 2) * 0.5 * 0.5

    # Entropy reward
    # entropy = - Sum_a policy(a|s) log policy(a|s) * entropy_cost
    entropy = jnp.mean(actor_network.entropy(policy_logits, rng))
    entropy_loss = entropy_cost * -entropy

    # Total loss = policy loss + value loss + entropy loss
    total_loss = policy_loss + v_loss + entropy_loss
    return total_loss, {
        'total_loss': total_loss,
        'policy_loss': policy_loss,
        'v_loss': v_loss,
        'entropy_loss': entropy_loss
    }

if __name__ == "__main__": 

    seed = 0

    import jax.random as jr
    from brax import envs

    def unit_test_gae_loss(seed):

        rng = jr.PRNGKey(seed); _rng, rng = jr.split(rng)

        truncation = jnp.array([
            [0,0],
            [0,0],
            [1,0],
            [0,0],
            [0,1],
        ])

        termination = jnp.array([
            [0,1],
            [0,0],
            [0,0],
            [0,0],
            [0,0],
        ])

        rewards = jr.normal(_rng, [5,2]); _rng, rng = jr.split(rng)
        values = jr.normal(_rng, [5,2]); _rng, rng = jr.split(rng)
        bootstrap_value = jr.normal(_rng, [2]); _rng, rng = jr.split(rng)

        gae_lambda = 1.0
        gae_discount = 0.99

        test_vs, test_advantages = compute_gae(truncation, termination, rewards, values, bootstrap_value, lambda_=gae_lambda, discount=gae_discount)

        truncation_mask = 1 - truncation

        # TD residuals:
        # delta_t = r_t + gamma * (1-d_t) * V(s_{t+1}) - V(s_t)
        # Append bootstrapped value to get [v1, ..., v_t+1]
        values_t_plus_1 = jnp.concatenate([values[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)
        deltas = rewards + gae_discount * (1 - termination) * values_t_plus_1 - values
        deltas *= truncation_mask

        # Initialization
        acc = jnp.zeros_like(bootstrap_value)
        vs_minus_v_xs = []

        # Reverse loop for GAE accumulation
        for t in reversed(range(truncation_mask.shape[0])):
            delta = deltas[t]
            trunc_mask = truncation_mask[t]
            term = termination[t]
            # Accumulate advantage
            acc = delta + gae_discount * (1 - term) * trunc_mask * gae_lambda * acc
            # Store the accumulated value
            vs_minus_v_xs.insert(0, acc)  # Insert at the front to reverse the order since we are iterating in reverse
        vs_minus_v_xs = jnp.stack(vs_minus_v_xs)

        # Final Values and Advantages
        # advantage = (rewards + gamma * (1-termination) * V(s_{t+1})) - V(s_t)
        # Add V(x_s) to get v_s.
        vs = jnp.add(vs_minus_v_xs, values)
        vs_t_plus_1 = jnp.concatenate([vs[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)
        advantages = (rewards + gae_discount * (1 - termination) * vs_t_plus_1 - values) * truncation_mask

    # unit_test_gae_loss(seed)

    def unit_test_ppo_loss(seed):

        rng = jr.PRNGKey(seed); _rng, rng = jr.split(rng)
    
        # need some arbitrary data
        num_envs = 3
        unroll_length = 5

        env = envs.get_environment(env_name="reacher", backend="positional")
        env_state = jax.vmap(env.reset)(jr.split(_rng, [num_envs]));  _rng, rng = jr.split(rng)

        actor_network = PPOStochasticActor(_rng, [env.observation_size, 32, 32, 32, env.action_size]);  _rng, rng = jr.split(rng)

        value_network = ValueNetwork(_rng, [env.observation_size, 32, 32, 32, 1]);  _rng, rng = jr.split(rng)

        data = []
        for i in range(unroll_length):

            action, raw_action = jax.vmap(actor_network)(jr.split(_rng, num=num_envs), env_state.obs); _rng, rng = jr.split(rng)
            nstate = jax.vmap(env.step)(env_state, action)

            data.append(
                Transition(
                    observation=env_state.obs,
                    action=action,
                    reward=nstate.reward,
                    discount=1 - nstate.done,
                    next_observation=nstate.obs,
                    extras = {
                        'policy_extras': {'raw_action': raw_action},
                        'state_extras': {}
                    }
                )
            )


        def f(carry, unused_t):

            observation = jax.vmap(env._get_obs)(env_state.pipeline_state) # a single snapshot across all environments
            action = jax.vmap(actor_network)(observation)
            env_state = jax.vmap(env.step)(env_state, action)
            reward = env_state.reward
            next_observation = jax.vmap(env._get_obs)(env_state.pipeline_state)

            actor_network()

        print('fin')

    unit_test_ppo_loss(seed)