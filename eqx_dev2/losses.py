import jax
import jax.numpy as jnp

from models import PPOStochasticActor, ValueNetwork
from data_structures import RunningMeanStd
from data_structures import ReplayBuffer

def compute_gae(truncation: jnp.ndarray,
                termination: jnp.ndarray,
                rewards: jnp.ndarray,
                values: jnp.ndarray,
                bootstrap_value: jnp.ndarray,
                lambda_: float = 1.0,
                discount: float = 0.99):
    """Calculates the Generalized Advantage Estimation (GAE).

    Args:
        truncation: A float32 tensor of shape [T, B] with truncation signal.
        termination: A float32 tensor of shape [T, B] with termination signal.
        rewards: A float32 tensor of shape [T, B] containing rewards generated by
        following the behaviour policy.
        values: A float32 tensor of shape [T, B] with the value function estimates
        wrt. the target policy.
        bootstrap_value: A float32 of shape [B] with the value function estimate at
        time T.
        lambda_: Mix between 1-step (lambda_=0) and n-step (lambda_=1). Defaults to
        lambda_=1.
        discount: TD discount.

    Returns:
        A float32 tensor of shape [T, B]. Can be used as target to
        train a baseline (V(x_t) - vs_t)^2.
        A float32 tensor of shape [T, B] of advantages.
    """
    # Used everywhere, mask = (1 - d_t)
    truncation_mask = 1 - truncation
    
    # TD residuals:
    # delta_t = r_t + gamma * (1-d_t) * V(s_{t+1}) - V(s_t)
    # Append bootstrapped value to get [v1, ..., v_t+1]
    values_t_plus_1 = jnp.concatenate([values[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)
    deltas = rewards + discount * (1 - termination) * values_t_plus_1 - values
    deltas *= truncation_mask

    # Recursive Advantage calculation
    # A_t = delta_t + gamma * lambda * (1-d_t) *A_{t+1}
    # when lambda = 1 all future steps are fully considered, when lambda = 0 only 1 step TD error is considered
    # acc is the accumulated advantage
    acc = jnp.zeros_like(bootstrap_value)
    vs_minus_v_xs = []
    def compute_vs_minus_v_xs(carry, target_t):
        lambda_, acc = carry
        truncation_mask, delta, termination = target_t
        acc = delta + discount * (1 - termination) * truncation_mask * lambda_ * acc
        return (lambda_, acc), (acc)
    (_, _), (vs_minus_v_xs) = jax.lax.scan(
        compute_vs_minus_v_xs, (lambda_, acc),
        (truncation_mask, deltas, termination),
        length=int(truncation_mask.shape[0]),
        reverse=True)
    
    # Final Values and Advantages
    # advantage = (rewards + gamma * (1-termination) * V(s_{t+1})) - V(s_t)
    # Add V(x_s) to get v_s.
    vs = jnp.add(vs_minus_v_xs, values)
    vs_t_plus_1 = jnp.concatenate([vs[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)
    advantages = (rewards + discount * (1 - termination) * vs_t_plus_1 - values) * truncation_mask
    return jax.lax.stop_gradient(vs), jax.lax.stop_gradient(advantages)


def compute_ppo_loss(
        policy: PPOStochasticActor, # Policy network (Equinox module)
        value: ValueNetwork,        # Value network (Equinox module)
        rms: RunningMeanStd,        # Running mean std parameters
        data: ReplayBuffer,      # Transition data
        rng: jnp.Array,
        entropy_cost: float = 1e-4,
        discounting: float = 0.99,
        reward_scaling: float = 1.0,
        gae_lambda: float = 0.95,
        clipping_epsilon: float = 0.2,
        normalize_advantage: bool = True
    ):

    # Put the time dimension first.
    data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)

    # Calculate the estimated advantages and values
    baseline = value(data.observation, rms=rms)
    bootstrap_value = value(data.next_observation[-1], rms=rms)
    rewards = data.reward * reward_scaling
    truncation = data.trunctation
    termination = (1 - data.discount) * (1 - truncation)
    vs, advantages = compute_gae(
        truncation=truncation,
        termination=termination,
        rewards=rewards,
        values=baseline,
        bootstrap_value=bootstrap_value,
        lambda_=gae_lambda,
        discount=discounting)
    
    # optional normalization
    if normalize_advantage:
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    # Calculate the Policy Ratio rho_s
    # rho_s = policy(a_t|s_t) / policy_old(a_t|s_t) = exp(log policy(a_t|s_t) - log policy_old(a_t|s_t))
    policy_logits = policy(data.observation, rms=rms)
    target_action_log_probs = policy.log_prob(policy_logits, data.raw_action)
    behaviour_action_log_probs = data.log_prob
    rho_s = jnp.exp(target_action_log_probs - behaviour_action_log_probs)

    # Surrogate Objective: we use a clipped version of the policy ratio to prevent large updates
    # L_surrogate = min(rho_t * A_t, clip(rho_t, 1-eps, 1+eps) * A_t)
    # where rho_t * A_t: the standard policy gradient
    # where clip(rho_t, 1-eps, 1+eps) * A_t: the clipped version which constrains ratio rho_t within [1-eps, 1+eps]
    surrogate_loss1 = rho_s * advantages
    surrogate_loss2 = jnp.clip(rho_s, 1 - clipping_epsilon, 1 + clipping_epsilon) * advantages

    # The final policy loss is the negative of the mean of the clipped surrogate objective
    policy_loss = -jnp.mean(jnp.minimum(surrogate_loss1, surrogate_loss2))

    # Value function loss
    # value_loss = 1/2 * 1/T * Sum_{t=0}^T (V_s - V(s_t)) ** 2
    # the loss is scaled y an additional factor 0.5 likely to balance the loss components
    v_error = vs - baseline
    v_loss = jnp.mean(v_error ** 2) * 0.5 * 0.5

    # Entropy reward
    # entropy = - Sum_a policy(a|s) log policy(a|s) * entropy_cost
    entropy = jnp.mean(policy.entropy(policy_logits, rng))
    entropy_loss = entropy_cost * -entropy

    # Total loss = policy loss + value loss + entropy loss
    total_loss = policy_loss + v_loss + entropy_loss
    return total_loss, {
        'total_loss': total_loss,
        'policy_loss': policy_loss,
        'v_loss': v_loss,
        'entropy_loss': entropy_loss
    }

if __name__ == "__main__": 

    pass

